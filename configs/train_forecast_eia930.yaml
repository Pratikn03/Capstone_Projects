task:
  horizon_hours: 24
  lookback_hours: 168
  quantiles: [0.1, 0.5, 0.9]
  targets: [load_mw, wind_mw, solar_mw]

seed: 42

# Advanced training options (enable via CLI flags):
# --enable-cv: 5-fold time-series cross-validation
# --use-pipeline: sklearn Pipeline for GBM (leakage-safe preprocessing)

data:
  processed_path: data/processed/us_eia930/features.parquet
  timestamp_col: timestamp

models:
  baseline_gbm:
    kind: lightgbm
    enabled: true
    params:
      n_estimators: 600
      learning_rate: 0.05
      max_depth: 10
      num_leaves: 128
      min_child_samples: 20
      subsample: 0.9
      colsample_bytree: 0.9
      reg_alpha: 0.05
      reg_lambda: 0.5
      verbosity: -1
  quantile_gbm:
    kind: lightgbm
    enabled: true
    params:
      n_estimators: 400
      learning_rate: 0.05
      max_depth: 8
      num_leaves: 64
      min_child_samples: 20
      subsample: 0.9
      colsample_bytree: 0.9
      verbosity: -1
  dl_lstm:
    kind: lstm
    enabled: true
    params:
      hidden_size: 128
      num_layers: 2
      dropout: 0.2
      epochs: 50
      batch_size: 256
  dl_tcn:
    kind: tcn
    enabled: true
    params:
      num_channels: [64, 64, 64]
      kernel_size: 3
      dropout: 0.2
      epochs: 50
      batch_size: 256

artifacts:
  out_dir: artifacts/models_eia930
reports:
  out_dir: reports/eia930

backtest:
  enabled: true
  out_path: reports/eia930/walk_forward_report.json

tuning:
  enabled: true
  engine: optuna
  n_trials: 50
  metric: val_loss
  direction: minimize
  params:
    baseline_gbm:
      n_estimators: {type: int, low: 100, high: 1000}
      learning_rate: {type: float, low: 0.01, high: 0.3, log: true}
    dl_lstm:
      hidden_size: {type: categorical, choices: [64, 128, 256]}
      dropout: {type: float, low: 0.0, high: 0.5}

tracking:
  enabled: true
  tool: mlflow # or wandb
  experiment_name: gridpulse_forecast
  log_models: true
  log_system_metrics: true

distributed:
  enabled: false
  backend: nccl # Options: nccl, gloo, mpi
  nproc_per_node: 1 # Set >1 for multi-GPU training
