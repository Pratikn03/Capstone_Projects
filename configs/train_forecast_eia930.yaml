task:
  horizon_hours: 24
  lookback_hours: 168
  quantiles: [0.1, 0.5, 0.9]
  targets: [load_mw, wind_mw, solar_mw]

seed: 42
seeds: [42, 123, 456, 789, 2024]  # Multiple seeds for ensemble

# Advanced training options:
# --enable-cv: 10-fold time-series cross-validation
# --use-pipeline: sklearn Pipeline for GBM (leakage-safe preprocessing)
# --ensemble: Train with multiple seeds and average predictions

cross_validation:
  enabled: true
  n_folds: 10
  strategy: time_series  # Preserves temporal ordering
  gap: 24  # 24-hour gap between train/val to prevent leakage

data:
  processed_path: data/processed/us_eia930/features.parquet
  timestamp_col: timestamp

models:
  baseline_gbm:
    kind: lightgbm
    enabled: true
    params:
      n_estimators: 1000
      learning_rate: 0.03
      max_depth: 12
      num_leaves: 256
      min_child_samples: 15
      subsample: 0.85
      colsample_bytree: 0.85
      reg_alpha: 0.1
      reg_lambda: 1.0
      verbosity: -1
      feature_fraction_seed: 42
      bagging_seed: 42
  quantile_gbm:
    kind: lightgbm
    enabled: true
    params:
      n_estimators: 400
      learning_rate: 0.05
      max_depth: 8
      num_leaves: 64
      min_child_samples: 20
      subsample: 0.9
      colsample_bytree: 0.9
      verbosity: -1
  dl_lstm:
    kind: lstm
    enabled: true
    params:
      hidden_size: 256
      num_layers: 3
      dropout: 0.3
      epochs: 100
      batch_size: 128
      learning_rate: 0.001
      weight_decay: 1e-5
      scheduler: cosine  # Cosine annealing LR
      warmup_epochs: 5
      early_stopping:
        enabled: true
        patience: 15
        min_delta: 0.0001
      gradient_clip: 1.0
  dl_tcn:
    kind: tcn
    enabled: true
    params:
      num_channels: [128, 128, 128, 64]
      kernel_size: 5
      dropout: 0.3
      epochs: 100
      batch_size: 128
      learning_rate: 0.001
      weight_decay: 1e-5
      scheduler: cosine
      warmup_epochs: 5
      early_stopping:
        enabled: true
        patience: 15
        min_delta: 0.0001
      gradient_clip: 1.0

artifacts:
  out_dir: artifacts/models_eia930
reports:
  out_dir: reports/eia930

backtest:
  enabled: true
  out_path: reports/eia930/walk_forward_report.json

tuning:
  enabled: true
  engine: optuna
  n_trials: 50
  metric: val_loss
  direction: minimize
  n_jobs: 4  # Parallel trials
  pruner: median  # Early stopping of bad trials
  params:
    baseline_gbm:
      n_estimators: {type: int, low: 500, high: 2000}
      learning_rate: {type: float, low: 0.005, high: 0.1, log: true}
      max_depth: {type: int, low: 8, high: 15}
      num_leaves: {type: int, low: 64, high: 512}
    dl_lstm:
      hidden_size: {type: categorical, choices: [128, 256, 512]}
      num_layers: {type: int, low: 2, high: 4}
      dropout: {type: float, low: 0.1, high: 0.5}
      learning_rate: {type: float, low: 0.0001, high: 0.01, log: true}
    dl_tcn:
      num_channels: {type: categorical, choices: [[64, 64, 64], [128, 128, 128], [128, 128, 128, 64]]}
      kernel_size: {type: int, low: 3, high: 7}
      dropout: {type: float, low: 0.1, high: 0.5}

tracking:
  enabled: true
  tool: mlflow # or wandb
  experiment_name: gridpulse_forecast
  log_models: true
  log_system_metrics: true

distributed:
  enabled: false
  backend: nccl # Options: nccl, gloo, mpi
  nproc_per_node: 1 # Set >1 for multi-GPU training
