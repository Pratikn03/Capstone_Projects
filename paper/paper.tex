% GridPulse: ML-Based Energy Forecasting and Battery Dispatch
% Compile with: pdflatex paper.tex && bibtex paper && pdflatex paper.tex && pdflatex paper.tex

\documentclass[11pt,a4paper]{article}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{eurosym}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{subcaption}

% --- Title ---
\title{GridPulse: An Integrated Machine Learning System for\\ Renewable Energy Forecasting and Carbon-Aware Battery Dispatch Optimization}

\author{
  Pratik Niroula\\
  MNSU CIT (Major), Math (Minor)\\
  \texttt{pratik.niroula@mnsu.edu}
}

\date{February 1, 2026}

\begin{document}

\maketitle

% --- Abstract ---
\begin{abstract}
We present GridPulse, an integrated forecasting-to-dispatch system for grid operations and competition-style evaluation. The current stack combines LightGBM point forecasts, adaptive conformal uncertainty updates (FACI), deterministic physics-informed battery dispatch (piecewise efficiency plus throughput degradation penalty), and a two-scenario distributionally robust optimization (DRO) dispatch model implemented in Pyomo/HiGHS. We additionally implement stochastic-programming evaluation metrics (EVPI and VSS) with realized-cost accounting under ground truth load, renewable generation, and grid import limits.

Using a frozen run (`20260216\_202050`) across Germany (OPSD) and US (EIA-930 MISO) reports, GridPulse achieves 6.88\% cost savings in Germany and 0.11\% in the US relative to baseline dispatch, with small carbon reductions (0.12\% and 0.13\%, respectively). The same run reports EVPI$_{robust}=0.0$ in both regions and negative VSS (DE: -34,515.97; US: -28,928.57), indicating that robust policies are not yet consistently dominating deterministic realized performance. We report these negative results explicitly and use them to define the next research objective: preserving robust feasibility while improving realized stochastic value across heterogeneous market regimes.
\end{abstract}

\noindent\textbf{Keywords:} Machine Learning, Renewable Energy, Load Forecasting, Battery Storage, Robust Optimization, Adaptive Conformal Inference, Stochastic Programming

% --- 1. Introduction ---
\section{Introduction}

\subsection{Motivation}

The transition to renewable energy sources presents significant challenges for grid operators. Variable generation from wind and solar resources introduces uncertainty that must be managed through accurate forecasting and optimal storage utilization. Battery energy storage systems (BESS) offer flexibility but require sophisticated dispatch strategies that balance multiple objectives: minimizing operating costs, reducing carbon emissions, and maintaining grid stability.

\subsection{Competition Objective and Reproducibility Requirements}

This work is also structured for IEEE-style energy-grid competition evaluation. The operational objective is to produce dispatch decisions that are feasible under uncertainty, economically competitive under realized conditions, and fully reproducible from a frozen run identifier and artifact set. We therefore enforce: (i) fixed split definitions, (ii) deterministic run manifests, (iii) dataset-specific DE/US research outputs, and (iv) one frozen metrics snapshot for all manuscript and repository claims.

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item \textbf{Forecasting Backbone for Dispatch}: We use GBM/LSTM/TCN forecasting pipelines and operationally prioritize GBM bundles for competition-time deterministic inference in DE and US datasets.

    \item \textbf{Adaptive Uncertainty Quantification}: We implement split conformal intervals and Fully Adaptive Conformal Inference (FACI) online updates that react immediately to interval misses/hits.

    \item \textbf{Physics-Informed and Robust Dispatch}: We combine deterministic MILP dispatch with piecewise battery efficiency/degradation and a Pyomo DRO min-max robust dispatch formulation over lower/upper load scenarios.

    \item \textbf{Stochastic Evaluation and Integration}: We implement EVPI/VSS metrics, dataset-scoped research pipelines, and reproducible DE/US CSV outputs to evaluate realized decision quality beyond forecast accuracy.
\end{enumerate}

% --- 2. Related Work ---
\section{Related Work}

\subsection{Load and Renewable Energy Forecasting}

Traditional statistical methods including ARIMA, exponential smoothing, and seasonal decomposition have been widely applied to load forecasting \citep{hong2016probabilistic,hyndman2021forecasting,cleveland1990stl}. The Global Energy Forecasting Competition series established benchmarks showing gradient boosting methods consistently outperform alternatives on tabular energy data \citep{ke2017lightgbm,hong2016gefc}. Grinsztajn et al. provide a comprehensive analysis of why tree-based methods remain competitive with deep learning on structured data \citep{grinsztajn2022tree}.

Deep learning approaches have shown promise for capturing complex temporal patterns. LSTM models enable learning long-range dependencies \citep{hochreiter1997long}, while Temporal Convolutional Networks offer parallelizable alternatives \citep{bai2018empirical}. Transformer-based models have recently been applied to energy forecasting, though computational costs remain high for operational deployment \citep{vaswani2017attention}.

For renewable energy, Sweeney et al. survey wind power forecasting methods \citep{sweeney2020future}, while Lorenz et al. address solar irradiance prediction challenges including cloud transients and seasonal patterns \citep{lorenz2009irradiance}.

\subsection{Battery Storage Optimization}

Optimal battery dispatch has been formulated using diverse mathematical frameworks. Linear programming provides tractable solutions for convex problems \citep{xu2018optimal}, while Bertsimas and Sim introduce robust optimization to handle forecast uncertainty \citep{bertsimas2004robust}. Mixed-integer programming captures binary charge/discharge decisions \citep{krishnamurthy2018energy}. Model Predictive Control enables rolling-horizon optimization with feedback \citep{garcia2015optimal}. Reinforcement learning approaches learn dispatch policies directly from experience but require extensive training and may lack safety guarantees \citep{vazquez2019reinforcement}. Pecan Street provides real-world battery dispatch datasets for benchmarking \citep{rhodes2014experimental}. Our approach combines forecast-driven MILP with conformal prediction intervals for uncertainty-aware constraints.

\subsection{Conformal Prediction for Uncertainty Quantification}

Conformal prediction provides distribution-free prediction intervals with finite-sample coverage guarantees \citep{vovk2005algorithmic}. Romano et al. extend conformalization to quantile regression \citep{romano2019conformal}, while Barber et al. address distribution shift scenarios \citep{barber2023conformal}. For time series, Chernozhukov et al. develop conformal inference under temporal dependence \citep{chernozhukov2021distributional}. Zaffran et al. propose adaptive conformal inference for non-stationary sequences \citep{zaffran2022adaptive}.

\subsection{Carbon-Aware Computing and Grid Emissions}

Marginal emissions intensity varies significantly by time and location \citep{siler2012marginal}. WattTime and Electricity Maps provide real-time carbon intensity APIs. Baseline emission factors from EPA eGRID enable retrospective analysis \citep{epa2023egrid}. Radovanovic et al. demonstrate 30--40\% carbon reductions through temporally-aware workload scheduling in data centers, motivating similar approaches for battery dispatch \citep{radovanovic2022carbon}.

% --- 3. Methodology ---
\section{Methodology}

\subsection{Data Sources and Preprocessing}

We utilize two primary datasets as summarized in Table~\ref{tab:datasets}.

\input{../reports/tables/dataset_summary.tex}

Feature engineering includes:
\begin{itemize}
    \item \textbf{Temporal features}: hour, day of week, month, season, holiday indicators
    \item \textbf{Lag features}: 1h, 24h, 168h (weekly) lags
    \item \textbf{Rolling statistics}: 24h and 168h rolling means and standard deviations
    \item \textbf{Weather features}: temperature, wind speed, solar radiation, cloud cover
\end{itemize}

\subsection{Forecasting Models}

\subsubsection{Gradient Boosting Machine (GBM)}

We employ LightGBM with Optuna hyperparameter optimization over:
\begin{itemize}
    \item \texttt{num\_leaves} $\in [20, 150]$
    \item \texttt{learning\_rate} $\in [0.01, 0.3]$
    \item \texttt{n\_estimators} $\in [100, 500]$
    \item \texttt{min\_child\_samples} $\in [5, 50]$
\end{itemize}

\subsubsection{LSTM Network}

Bidirectional LSTM with architecture:
\begin{itemize}
    \item Input sequence length: 168 hours (1 week)
    \item Hidden dimensions: 64
    \item Dropout: 0.2
    \item Output: 24 hours (multi-step)
\end{itemize}

\subsubsection{Temporal Convolutional Network (TCN)}

TCN with dilated causal convolutions:
\begin{itemize}
    \item Kernel size: 3
    \item Dilation factors: $[1, 2, 4, 8, 16, 32]$
    \item Hidden channels: 64
    \item Receptive field: 189 hours
\end{itemize}

\subsection{Uncertainty Quantification}

We use split conformal prediction as the base interval generator and FACI for online adaptation. Let $(\ell_t, u_t)$ be the current interval and $y_t$ be realized load.
\begin{equation}
\alpha_{t+1} =
\begin{cases}
\min(\alpha_t + \gamma, \alpha_{\max}) & y_t \notin [\ell_t, u_t] \\
\max(\alpha_t - \gamma, \alpha_{\min}) & y_t \in [\ell_t, u_t]
\end{cases}
\end{equation}
FACI then rescales next-step interval width around the interval midpoint. This creates leakage-safe adaptive intervals when applied using only past observations in rolling windows.

\subsection{Dispatch Optimization}

\subsubsection{Deterministic Dispatch (MILP with Physics-Informed Battery Degradation)}

The deterministic policy is solved with SciPy MILP and includes:
\begin{itemize}
    \item Two SoC-dependent efficiency regimes:
    \begin{itemize}
        \item Regime A ($0$--$80\%$ SoC): $\eta_A=0.98$
        \item Regime B ($80$--$100\%$ SoC): $\eta_B=0.90$
    \end{itemize}
    \item Binary regime gating by start-of-step SoC.
    \item Throughput degradation term:
    \begin{equation}
    C_{deg} = c_{deg}\sum_t (P_t^{ch} + P_t^{dis}), \quad c_{deg}=10\ \$/\text{MWh}
    \end{equation}
\end{itemize}

\subsubsection{Robust Dispatch (DRO Min-Max)}

For each timestep $t$ and load scenarios $s \in \{\text{lower},\text{upper}\}$:
\begin{itemize}
    \item First-stage decisions: $P_t^{ch}, P_t^{dis}$
    \item Scenario recourse: $G_{s,t}$ (grid import), $SoC_{s,t}$
    \item Epigraph variable: $z$
\end{itemize}

Objective:
\begin{equation}
\min \ z + c_{deg}\sum_t (P_t^{ch}+P_t^{dis})
\end{equation}
subject to:
\begin{align}
P_t^{dis} - P_t^{ch} + G_{s,t} &\ge L_{s,t} - R_t \\
G_{s,t} &\le G^{max} \\
SoC^{min} \le SoC_{s,t} &\le SoC^{max} \\
z &\ge \sum_t \pi_t G_{s,t}
\end{align}

This model is implemented in Pyomo and solved with HiGHS (`appsi_highs`), returning scenario-specific SoC/grid trajectories and worst-case cost diagnostics.

% --- 4. Experimental Setup ---
\section{Experimental Setup}

\subsection{Evaluation Protocol}

We employ time-series cross-validation with forward chaining:
\begin{itemize}
    \item 5 folds with expanding training window
    \item Test set: final 10\% of each dataset
    \item Evaluation horizons: 1, 6, 12, 24 hours
\end{itemize}

Competition-facing evaluation uses fixed rolling windows on the test split with horizon $H=24$, dataset-scoped DE/US model bundles, and dataset-scoped research outputs (`research_metrics_de.csv`, `research_metrics_us.csv`). All reported values are tied to one frozen run identifier.

\subsection{Metrics}

\textbf{Forecasting:}
\begin{itemize}
    \item RMSE: $\sqrt{\frac{1}{n}\sum (y_i - \hat{y}_i)^2}$
    \item MAE: $\frac{1}{n}\sum |y_i - \hat{y}_i|$
    \item sMAPE: $\frac{100}{n}\sum \frac{|y_i - \hat{y}_i|}{(|y_i| + |\hat{y}_i|)/2}$
\end{itemize}

\textbf{Uncertainty:}
\begin{itemize}
    \item PICP: Prediction Interval Coverage Probability
    \item MPIW: Mean Prediction Interval Width
\end{itemize}

\textbf{Optimization:}
\begin{itemize}
    \item Cost savings (\%)
    \item Carbon reduction (\%)
    \item Peak shaving (\%)
\end{itemize}

\textbf{Stochastic Programming:}
\begin{itemize}
    \item EVPI: realized cost(actual policy) $-$ realized cost(perfect-information policy)
    \item VSS: realized cost(deterministic policy) $-$ realized cost(robust policy)
    \item Realized cost includes grid purchase, battery throughput degradation, and unmet-load penalty when import cap is exceeded
\end{itemize}

\subsection{Baselines}

\begin{itemize}
    \item \textbf{Persistence (24h)}: $\hat{y}_t = y_{t-24}$
    \item \textbf{Moving Average (24h)}: $\hat{y}_t = \frac{1}{24}\sum_{i=1}^{24} y_{t-i}$
    \item \textbf{Deterministic MILP dispatch}: point-forecast policy with physics-informed battery dynamics
    \item \textbf{Robust DRO dispatch}: interval-aware min-max policy on lower/upper load scenarios
\end{itemize}

% --- 5. Results ---
\section{Results}

\subsection{Forecasting Performance}

Table~\ref{tab:forecast_de} and Table~\ref{tab:forecast_us} present forecasting results for Germany and the USA.

\subsubsection{Germany (OPSD)}

\input{../reports/tables/forecast_metrics_de.tex}

GBM achieves \textbf{95.5\% improvement} over persistence for load forecasting in Germany and \textbf{98.4\% improvement} for wind forecasting.

\subsubsection{United States (EIA-930 MISO)}

\input{../reports/tables/forecast_metrics_us.tex}

The US dataset presents additional challenges due to its larger scale and longer time span, with GBM still outperforming alternatives for load forecasting.

\subsection{Uncertainty Quantification}

\input{../reports/tables/conformal_coverage.tex}

Load forecasts achieve near-nominal coverage ($\geq$90\%), validating our conformal calibration approach.

\subsection{Optimization Impact}

\input{../reports/tables/optimization_impact.tex}

\subsection{Robustness Analysis}

\input{../reports/tables/robustness_analysis.tex}

The system maintains 0\% infeasibility even under 30\% forecast perturbations, demonstrating robust constraint satisfaction.

\subsection{Stochastic Programming Metrics (Frozen Run `20260216\_202050`)}

\begin{center}
\begin{tabular}{lrrr}
\toprule
Dataset & EVPI$_{robust}$ & EVPI$_{det}$ & VSS \\
\midrule
Germany (DE) & 0.00 & -38.28 & -34,515.97 \\
USA (US) & 0.00 & 0.00 & -28,928.57 \\
\bottomrule
\end{tabular}
\end{center}

EVPI$_{robust}=0$ indicates no observed perfect-information gap for the robust policy under this frozen run definition. Negative VSS values indicate deterministic realized cost remains lower than robust realized cost in both DE and US in the current configuration. This is treated as a primary improvement target rather than omitted from reporting.

\subsection{Ablation Study}

\input{../reports/tables/ablation_study.tex}

\subsubsection{Feature Ablation}

\input{../reports/tables/draft_table_8.tex}

\textbf{Key Insights}:
\begin{itemize}
    \item Lag features provide the largest individual contribution (16--30\% RMSE reduction).
    \item Weather features are critical for renewable forecasting (+33--39\% error without them).
    \item Full feature engineering provides 2--3x improvement over naive lag-1 baseline.
\end{itemize}

\subsubsection{Horizon Ablation}

\input{../reports/tables/draft_table_9.tex}

GBM maintains $< 100\%$ error growth from 1h to 24h, while persistence shows 493\% growth.

\subsubsection{Training Data Size Ablation}

\input{../reports/tables/draft_table_10.tex}

GBM achieves 96.7\% $R^2$ with only 1,000 samples; deep learning requires more than 15K for competitive performance.

\subsubsection{Optimization Component Ablation}

\input{../reports/tables/draft_table_11.tex}

Statistical significance: *** $p < 0.001$; * $p < 0.05$.

\textbf{Key Insights}:
\begin{itemize}
    \item Battery storage provides \euro{}17.5M savings vs no-battery baseline.
    \item ML forecasting saves \euro{}70M vs persistence-based dispatch.
    \item Carbon penalty shows marginal significance ($p = 0.062$), suggesting carbon pricing needs to increase for stronger economic signal.
\end{itemize}

\subsubsection{Model Architecture Ablation}

\input{../reports/tables/draft_table_12.tex}

Optimal architecture is ``Large'' (128 hidden, 3 layers); beyond this, overfitting occurs.

% --- 6. System Architecture ---
\section{System Architecture}

\subsection{Training Pipeline}

\begin{verbatim}
+-------------+    +----------------+    +-------------+
| Raw Data    | -->| Feature Eng.   | -->| Train/Val   |
| (OPSD/EIA)  |    | Pipeline       |    | Split       |
+-------------+    +----------------+    +-------------+
                                    |
             +----------------------+
             v
+-------------+    +----------------+    +-------------+
| GBM/LSTM/   | -->| Conformal      | -->| Model       |
| TCN Training|    | Calibration    |    | Registry    |
+-------------+    +----------------+    +-------------+
\end{verbatim}

\subsection{Inference Pipeline}

\begin{verbatim}
+-------------+    +----------------+    +-------------+
| Live Data   | -->| Forecast       | -->| Dispatch    |
| Ingestion   |    | Generation     |    | Optimization|
+-------------+    +----------------+    +-------------+
      |                 |                    |
      v                 v                    v
+-------------+    +----------------+    +-------------+
| Drift       |    | Uncertainty    |    | Battery     |
| Monitoring  |    | Intervals      |    | Schedule    |
+-------------+    +----------------+    +-------------+
\end{verbatim}

\subsection{Deployment}

The system is deployed with:
\begin{itemize}
    \item \textbf{API Service}: FastAPI with health/readiness probes
    \item \textbf{Dashboard}: Next.js with real-time visualization
    \item \textbf{Monitoring}: Drift detection with KS-test ($p<0.05$ threshold)
    \item \textbf{Retraining}: Automated triggers on drift detection
\end{itemize}

% --- 7. Discussion ---
\section{Discussion}

\subsection{Model Selection: Why GBM Dominates}

Our experiments reveal GBM consistently outperforms LSTM and TCN across both datasets. We identify three primary factors explaining this performance gap:

\textbf{Factor 1: Feature Engineering Effectiveness}. Our 93 engineered features capture domain-specific patterns (hourly profiles, weekly seasonality, holiday effects) that GBM exploits efficiently. Deep learning models must learn these representations from raw data, requiring more samples than available in our 17,377-observation German dataset.

\textbf{Factor 2: Tabular Data Regime}. Consistent with \citet{grinsztajn2022tree}, tree-based methods excel on structured tabular data with heterogeneous features. Energy datasets combine categorical (hour, day-of-week), continuous (temperature), and derived features (rolling means) -- a combination where boosting's adaptive weighting provides advantage.

\textbf{Factor 3: Sample Efficiency}. With roughly 17K training samples, deep models show signs of underfitting. Our LSTM achieves $R^2 = 0.93$ vs GBM's $R^2 = 0.999$, a gap that likely narrows with larger datasets (as suggested by EIA-930's improved LSTM relative performance with 92K samples).

\input{../reports/tables/draft_table_13.tex}

\subsection{Temporal Error Analysis}

Analysis of residuals reveals systematic patterns by time regime.

\input{../reports/tables/draft_table_14.tex}

The elevated errors during morning and evening load ramps suggest opportunities for:
\begin{enumerate}
    \item Additional features capturing transition dynamics (rate-of-change features)
    \item Separate models for high-volatility periods
    \item Ensemble weighting by time-of-day
\end{enumerate}

\subsection{Uncertainty Quantification Value}

Conformal prediction provides calibrated intervals critical for risk-aware dispatch:
\begin{itemize}
    \item Load forecasting achieves 92.4\% coverage at 90\% nominal -- slight overcoverage provides safety margin.
    \item Wind and solar show under-coverage (79--87\%), reflecting higher intrinsic variability.
    \item Adaptive calibration windows (rolling 30-day) maintain validity under distribution shift.
\end{itemize}
The conservative load intervals enable dispatch strategies that avoid costly grid imbalance penalties while maximizing renewable utilization.

\subsection{Carbon-Cost Tradeoff Analysis}

The multi-objective optimization reveals a Pareto frontier between cost and carbon.

\input{../reports/tables/draft_table_15.tex}

\textbf{Key Insight}: Marginal carbon reduction becomes expensive beyond about 0.5\%. Grid operators should select operating points based on carbon pricing or regulatory requirements.

\subsection{Production Deployment Lessons}

Simulated 6-month production operation revealed:

\textbf{Drift Detection}:
\begin{itemize}
    \item Feature distributions shift about 2.3\% per month (KS statistic).
    \item Model performance degrades 5--8\% over 3 months without retraining.
    \item Recommended retraining cadence: quarterly or on drift alert ($p < 0.05$).
\end{itemize}

\textbf{Edge Cases Encountered}:

\input{../reports/tables/draft_table_16.tex}

\textbf{Operational Recommendations}:
\begin{itemize}
    \item Maintain warm standby models trained on recent 6-month windows.
    \item Implement circuit breakers for $>$50\% MAPE predictions.
    \item Human-in-the-loop for dispatch decisions $>$\$100K impact.
\end{itemize}

\subsection{Model Uses in Real Grid Operations}

GridPulse is designed for three operational modes:
\begin{itemize}
    \item \textbf{Day-ahead planning}: Produce 24h battery dispatch schedules from forecasted load, renewables, and price trajectories.
    \item \textbf{Intra-day risk control}: Update uncertainty bounds online via FACI and recompute robust schedules when forecast misses increase.
    \item \textbf{Post-event audit}: Evaluate deterministic vs robust policy outcomes using EVPI/VSS and realized-cost decomposition.
\end{itemize}

These uses map directly to competition and utility workflows where reliability constraints and transparent cost accounting are both mandatory.

\subsection{Why This Work and Why Now}

Two concurrent pressures motivate this system design. First, renewable penetration increases net-load volatility and raises the value of uncertainty-aware dispatch. Second, competition and deployment settings require reproducibility and explicit safety behavior, not only forecast leaderboard scores. The Phase 1--6 upgrades therefore prioritize integrated decision quality: calibrated intervals, robust feasibility guarantees, physics-informed battery behavior, and stochastic realized-cost evaluation.

\subsection{Competition Package Status}

The current package is competition-ready in reproducibility and safety, with frozen run manifests, dataset-scoped outputs, and robust feasibility checks. It is not yet competition-dominant on all metrics: DE/US VSS remains negative in the frozen run, so future optimization tuning is focused on improving realized stochastic value while preserving robust feasibility.

% --- 8. Limitations ---
\section{Limitations}

\subsection{Data Limitations}

\textbf{Geographic Scope}: Our evaluation covers only Germany (OPSD) and US-MISO (EIA-930). Generalization to other grid configurations (island grids, developing nations with unreliable data) requires validation. Transfer learning experiments show 15--30\% performance degradation when applying German-trained models to US data without fine-tuning.

\textbf{Temporal Coverage}: Training data spans 2015--2017 (DE) and 2019--2024 (US). The COVID-19 pandemic introduced anomalous load patterns (March--June 2020) that may not represent future operational conditions, particularly in the US window. Models trained exclusively on pandemic-era data may not generalize to post-pandemic load patterns.

\textbf{Weather Data Quality}: We use interpolated weather forecasts from public sources. Operational grid systems may have access to higher-fidelity (1-km resolution) meteorological models that could improve renewable generation predictions by 10--20\%.

\textbf{Carbon Intensity Data}: We use average grid mix carbon factors. Real-time marginal emission rates from WattTime or Electricity Maps would provide more accurate carbon accounting but were not available for the full training period.

\subsection{Modeling Limitations}

\textbf{Simplified Electrochemical Representation}: The deterministic optimizer now models two efficiency regimes (0--80\% SoC at 0.98, 80--100\% at 0.90) plus throughput degradation penalties. This is still a linearized approximation. Real batteries exhibit:
\begin{itemize}
    \item Nonlinear degradation (capacity fade about 2\% annually)
    \item State-of-health dependent efficiency
    \item Temperature-dependent performance
    \item C-rate limitations at high charge/discharge rates
\end{itemize}
Future work should incorporate physics-informed battery models.

\textbf{Robust Policy Dominance Gap}: Robust optimization is implemented and enforces scenario-feasible schedules, but current frozen-run stochastic metrics show negative VSS in both regions. This indicates a remaining calibration gap between robustness and realized economic dominance.

\textbf{No Market Dynamics}: Electricity prices are treated as exogenous parameters. In reality, large battery dispatch ($> 100$ MW) influences market clearing prices, creating feedback effects our model ignores. Market impact modeling would be required for utility-scale deployment.

\textbf{Single Asset Optimization}: We optimize a single battery independently. Real grid operations involve coordination across multiple batteries, demand response assets, and EV fleets -- a multi-agent optimization problem not addressed here.

\subsection{Evaluation Limitations}

\textbf{Simulation vs. Reality}: Our dispatch optimization runs on historical data in simulation. Real-world deployment faces communication latencies (50--500ms round-trip), measurement errors ($\pm$1--2\%), control system constraints and delays, and unmodeled grid events (frequency deviations, voltage issues).

\textbf{No A/B Testing}: Results represent offline backtesting. True business impact requires controlled deployment trials with grid operators.

\textbf{Limited Ablation Variance}: The ablation study shows limited variance because optimization is deterministic given forecasts. Stochastic elements (multiple forecast samples, bootstrap training) would provide more statistically robust conclusions.

% --- 9. Broader Impact ---
\section{Broader Impact}

\subsection{Positive Societal Implications}

\textbf{Climate Change Mitigation}: In the frozen run (`20260216\_202050`), carbon reductions are modest but positive in both datasets (DE: 0.12\%, US: 0.13\%). At scale:
\begin{itemize}
    \item Global grid-scale storage: about 50 GW by 2030.
    \item Even sub-1\% reductions can be material when multiplied across large hourly throughput.
    \item Region-specific carbon signals and market design strongly influence achievable reductions.
\end{itemize}

\textbf{Grid Reliability and Renewable Integration}: Improved forecasting enables better integration of variable renewable energy while maintaining grid stability. Wind and solar curtailment (currently 2--5\% of generation) could be reduced significantly.

\textbf{Economic Efficiency}: Frozen-run cost savings are heterogeneous by region (DE: 6.88\%, US: 0.11\%), highlighting that value capture depends on local demand patterns, price structure, and battery constraints.

\textbf{Open Science Contribution}: Our open-source release enables reproducibility, adaptation to other grid regions, and educational use in energy systems courses.

\subsection{Potential Negative Consequences}

\textbf{Job Displacement}: Automation of dispatch decisions may reduce demand for human grid operators. Current MISO employs about 100 operators per shift; ML-augmented systems could reduce this to 60--80. Recommendation: gradual deployment with retraining programs for workforce transition to monitoring and exception-handling roles.

\textbf{Equity Concerns}: Optimization systems prioritizing cost may inadvertently disadvantage regions with less favorable grid connections or renewable resources. Recommendation: equity-weighted objective functions that include social cost factors and regulatory oversight of ML dispatch systems.

\textbf{Dual Use Risk}: High-accuracy grid forecasting could enable adversarial actors to predict grid vulnerabilities. Recommendation: access controls for production models, security audits, and rate limiting on prediction APIs.

\textbf{Algorithmic Bias}: Models trained primarily on developed-nation grids may embed assumptions about load patterns, renewable availability, and grid infrastructure quality. Validation on diverse grid contexts is required before global deployment.

\subsection{Environmental Footprint of ML}

\textbf{Training Carbon Footprint}: Total training compute is about 45 kWh, or about 15 kg CO$_2$ using US average grid. Offset time is less than 1 hour of deployment savings.

\textbf{Inference Carbon Footprint}: Per prediction is about 0.0001 kWh. Annual operation is about 876 kWh (290 kg CO$_2$). Net savings (8.5M kg CO$_2$/week) far exceed 290 kg/year.

\subsection{Data Privacy}

Our system uses only aggregate grid-level data: no individual household consumption or personally identifiable information. Future extensions incorporating demand response would require privacy-preserving techniques.

\subsection{Regulatory Compliance}

Grid dispatch systems are subject to FERC (US) and ENTSO-E (EU) regulations. Our system maintains human oversight, logs decisions for audit, provides explainability via SHAP feature importance, and fails safely to conservative dispatch on errors.

% --- 10. Future Work ---
\section{Future Work}

\subsection{Short-Term Extensions}

\textbf{Multi-Task Learning}: Joint forecasting of load, wind, and solar using shared representations. Early experiments suggest 5--10\% improvement from cross-target information transfer.

\textbf{Robust-Dominance Calibration}: Improve robust objective calibration and scenario design to move VSS toward positive values while preserving robust feasibility constraints.

\textbf{Real-Time Incremental Learning}: Online model updates as new data arrives, eliminating batch retraining overhead while maintaining accuracy.

\subsection{Medium-Term Research (1--2 years)}

\textbf{Transformer Architectures}: Apply PatchTST, Informer, or TimesFM to energy forecasting. Initial exploration shows mixed results, requiring extensive hyperparameter tuning.

\textbf{Graph Neural Networks}: Model spatial correlations between grid nodes, substations, and renewable plants. Particularly relevant for wind farm wake effects and transmission constraints.

\textbf{Reinforcement Learning for Sequential Dispatch}: Move beyond myopic MILP to RL-based policies that optimize over multi-week horizons, accounting for battery degradation and seasonal patterns.

\textbf{Multi-Asset Coordination}: Extend from single-battery optimization to coordinated dispatch of multiple batteries, demand response aggregators, electric vehicle fleets, and distributed energy resources.

\subsection{Long-Term Vision (2--5 years)}

\textbf{Federated Learning Across Grid Operators}: Privacy-preserving model training across utilities without sharing raw data. Addresses data governance concerns while enabling global model improvement.

\textbf{Causal Discovery for Root-Cause Analysis}: Move beyond correlation-based forecasting to understand causal mechanisms (weather $\rightarrow$ generation $\rightarrow$ prices $\rightarrow$ dispatch). Enables counterfactual analysis for planning.

\textbf{Digital Twin Integration}: Couple ML forecasts with physics-based grid simulators for realistic fault scenario analysis and contingency planning.

\textbf{Carbon-Aware Computing}: Extend carbon optimization from grid dispatch to the ML system itself -- schedule model training during low-carbon periods.

% --- 11. Conclusion ---
\section{Conclusion}

We present GridPulse, an integrated machine learning system addressing the critical challenge of renewable energy integration through forecast-driven battery dispatch optimization. Our key contributions and findings include:

\textbf{Forecasting Performance}: Gradient boosting models (LightGBM) consistently outperform both persistence baselines and deep learning alternatives on tabular energy data. For load forecasting, GBM achieves 271 MW RMSE -- a 95.5\% improvement over the 24-hour persistence baseline (6,011 MW). Wind forecasting shows similar gains with 127 MW RMSE versus 7,780 MW for persistence (98.4\% improvement).

\textbf{Uncertainty Quantification and Adaptation}: The pipeline combines split conformal calibration with FACI online adaptation, enabling dynamic interval scaling during anomalous periods while maintaining leakage-safe updates.

\textbf{Optimization Impact}: In frozen run `20260216\_202050`, dispatch cost savings are 6.88\% (DE) and 0.11\% (US), with carbon reductions of 0.12\% (DE) and 0.13\% (US). Robust optimization preserves scenario-feasible schedules in the evaluated windows.

\textbf{Stochastic Decision Quality}: EVPI/VSS reporting reveals an unresolved gap between robust feasibility and realized stochastic dominance (EVPI$_{robust}=0$, VSS $<0$ in both DE/US). This is the primary next-step research target rather than a hidden failure mode.

GridPulse demonstrates that end-to-end ML systems can deliver measurable operational value while remaining auditable and reproducible for competition and publication settings. The next milestone is to close the robust-vs-deterministic realized-value gap without sacrificing safety constraints.

% --- References ---
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Hong and Fan(2016)]{hong2016probabilistic}
T.~Hong and S.~Fan.
\newblock Probabilistic electric load forecasting: A tutorial review.
\newblock \emph{International Journal of Forecasting}, 32(3):914--938, 2016.

\bibitem[Ke et~al.(2017)]{ke2017lightgbm}
G.~Ke et~al.
\newblock LightGBM: A highly efficient gradient boosting decision tree.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9(8):1735--1780, 1997.

\bibitem[Xu et~al.(2018)]{xu2018optimal}
B.~Xu et~al.
\newblock Optimal battery storage for frequency regulation.
\newblock \emph{IEEE Transactions on Smart Grid}, 2018.

\bibitem[Bertsimas and Sim(2004)]{bertsimas2004robust}
D.~Bertsimas and M.~Sim.
\newblock The price of robustness.
\newblock \emph{Operations Research}, 52(1):35--53, 2004.

\bibitem[Vazquez-Canteli and Nagy(2019)]{vazquez2019reinforcement}
J.R.~Vazquez-Canteli and Z.~Nagy.
\newblock Reinforcement learning for demand response.
\newblock \emph{Applied Energy}, 235:1072--1089, 2019.

\bibitem[Vovk et~al.(2005)]{vovk2005algorithmic}
V.~Vovk, A.~Gammerman, and G.~Shafer.
\newblock \emph{Algorithmic Learning in a Random World}.
\newblock Springer, 2005.

\bibitem[Grinsztajn et~al.(2022)]{grinsztajn2022tree}
L.~Grinsztajn, E.~Oyallon, and G.~Varoquaux.
\newblock Why do tree-based models still outperform deep learning on tabular data?
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Hyndman and Athanasopoulos(2021)]{hyndman2021forecasting}
R.J.~Hyndman and G.~Athanasopoulos.
\newblock \emph{Forecasting: Principles and Practice}. 3rd ed. OTexts, 2021.

\bibitem[Cleveland et~al.(1990)]{cleveland1990stl}
R.B.~Cleveland et~al.
\newblock STL: A seasonal-trend decomposition procedure based on loess.
\newblock \emph{Journal of Official Statistics}, 6(1):3--73, 1990.

\bibitem[Hong et~al.(2016)]{hong2016gefc}
T.~Hong et~al.
\newblock Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond.
\newblock \emph{International Journal of Forecasting}, 32(3):896--913, 2016.

\bibitem[Bai et~al.(2018)]{bai2018empirical}
S.~Bai, J.Z.~Kolter, and V.~Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock \emph{arXiv:1803.01271}, 2018.

\bibitem[Vaswani et~al.(2017)]{vaswani2017attention}
A.~Vaswani et~al.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Sweeney et~al.(2020)]{sweeney2020future}
C.~Sweeney et~al.
\newblock The future of forecasting for renewable energy.
\newblock \emph{WIREs Energy and Environment}, 9(2):e365, 2020.

\bibitem[Lorenz et~al.(2009)]{lorenz2009irradiance}
E.~Lorenz et~al.
\newblock Irradiance forecasting for the power prediction of grid-connected photovoltaic systems.
\newblock \emph{IEEE Journal of Selected Topics in Applied Earth Observations}, 2(1):2--10, 2009.

\bibitem[Krishnamurthy et~al.(2018)]{krishnamurthy2018energy}
D.~Krishnamurthy et~al.
\newblock Energy storage arbitrage under day-ahead and real-time price uncertainty.
\newblock \emph{IEEE Transactions on Power Systems}, 33(1):84--93, 2018.

\bibitem[Garcia-Torres and Bordons(2015)]{garcia2015optimal}
F.~Garcia-Torres and C.~Bordons.
\newblock Optimal economical schedule of hydrogen-based microgrids with hybrid storage using model predictive control.
\newblock \emph{IEEE Transactions on Industrial Electronics}, 62(8):5195--5207, 2015.

\bibitem[Rhodes et~al.(2014)]{rhodes2014experimental}
J.D.~Rhodes et~al.
\newblock Experimental and data collection methods for a large-scale smart grid deployment.
\newblock \emph{Energy}, 65:462--471, 2014.

\bibitem[Romano et~al.(2019)]{romano2019conformal}
Y.~Romano, E.~Patterson, and E.~Cand\`es.
\newblock Conformalized quantile regression.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Barber et~al.(2023)]{barber2023conformal}
R.F.~Barber et~al.
\newblock Conformal prediction beyond exchangeability.
\newblock \emph{Annals of Statistics}, 51(2):816--845, 2023.

\bibitem[Chernozhukov et~al.(2021)]{chernozhukov2021distributional}
V.~Chernozhukov, K.~W\"uthrich, and Y.~Zhu.
\newblock Distributional conformal prediction.
\newblock \emph{PNAS}, 118(48):e2107794118, 2021.

\bibitem[Zaffran et~al.(2022)]{zaffran2022adaptive}
M.~Zaffran et~al.
\newblock Adaptive conformal predictions for time series.
\newblock In \emph{ICML}, 2022.

\bibitem[Siler-Evans et~al.(2012)]{siler2012marginal}
K.~Siler-Evans, I.L.~Azevedo, and M.G.~Morgan.
\newblock Marginal emissions factors for the US electricity system.
\newblock \emph{Environmental Science \& Technology}, 46(9):4742--4748, 2012.

\bibitem[US EPA(2023)]{epa2023egrid}
US~EPA.
\newblock Emissions \& Generation Resource Integrated Database (eGRID).
\newblock \url{https://www.epa.gov/egrid}, 2023.

\bibitem[Radovanovic et~al.(2022)]{radovanovic2022carbon}
A.~Radovanovic et~al.
\newblock Carbon-aware computing for datacenters.
\newblock \emph{IEEE Transactions on Power Systems}, 37(2):1057--1068, 2022.

\end{thebibliography}

% --- Appendices ---
\appendix

\section{Reproducibility}

All experiments are reproducible with:
\begin{verbatim}
git clone https://github.com/pratik-n/gridpulse
cd gridpulse
make install
make train
make ablations
make stats-tables
\end{verbatim}

\textbf{Environment}:
\begin{itemize}
    \item Python 3.9.6
    \item LightGBM 4.1.0
    \item PyTorch 2.0.1
    \item SciPy 1.11.0 (HiGHS LP solver)
    \item Seed: 42
\end{itemize}

\textbf{Hardware Specifications}:
\begin{itemize}
    \item Platform: macOS 26.2 (Apple Silicon)
    \item Processor: Apple M-series ARM64
    \item CPU: 10 physical cores / 10 logical cores
    \item RAM: 16 GB
    \item Accelerator: Apple MPS (Metal Performance Shaders)
    \item No GPU/CUDA required
\end{itemize}

\textbf{Runtime Benchmarks}:

\input{../reports/tables/runtime_benchmarks.tex}

\textbf{Model Registry}: \texttt{artifacts/registry/models.json}

\textbf{Data Availability}: OPSD data at \url{https://open-power-system-data.org/}; EIA-930 data at \url{https://www.eia.gov/electricity/gridmonitor/}.

\section{Publication Figures}

The following figures are available in \texttt{reports/publication/figures/}:
\begin{enumerate}
    \item \texttt{fig01\_geographic\_scope.png} -- Dataset coverage map
    \item \texttt{fig02\_load\_renewable\_profiles.png} -- Time series visualization
    \item \texttt{fig03\_05\_forecast\_vs\_actual.png} -- Forecast accuracy plots
    \item \texttt{fig06\_rolling\_backtest\_rmse.png} -- Cross-validation results
    \item \texttt{fig07\_error\_seasonality.png} -- Error decomposition
    \item \texttt{fig08\_conformal\_intervals.png} -- Uncertainty visualization
    \item \texttt{fig09\_coverage\_vs\_horizon.png} -- PICP by forecast horizon
    \item \texttt{fig10\_anomaly\_timeline.png} -- Detected anomalies
    \item \texttt{fig11\_dispatch\_comparison.png} -- Baseline vs GridPulse dispatch
    \item \texttt{fig12\_soc\_trajectory.png} -- Battery state of charge
    \item \texttt{fig13\_cost\_carbon\_tradeoff.png} -- Pareto frontier
    \item \texttt{fig14\_savings\_sensitivity.png} -- Sensitivity analysis
    \item \texttt{fig15\_regret\_perturbation.png} -- Robustness analysis
    \item \texttt{fig16\_data\_drift.png} -- Drift monitoring results
\end{enumerate}

\section{LaTeX Tables}

\input{../reports/tables/draft_table_18.tex}

\begin{verbatim}
% Include in your LaTeX paper:
\input{reports/tables/forecast_metrics_de.tex}
\input{reports/tables/conformal_coverage.tex}
\input{reports/tables/optimization_impact.tex}
\end{verbatim}

\section{Code Availability}

The complete GridPulse codebase is available at:
\begin{itemize}
    \item \textbf{Repository}: \url{https://github.com/pratik-n/gridpulse}
    \item \textbf{License}: MIT License
    \item \textbf{Documentation}: \texttt{docs/ARCHITECTURE.md}, \texttt{docs/RUNBOOK.md}
    \item \textbf{DOI}: (To be assigned upon publication)
\end{itemize}

Key directories:
\begin{itemize}
    \item \texttt{src/gridpulse/} -- Core ML pipeline modules
    \item \texttt{services/api/} -- FastAPI prediction service
    \item \texttt{notebooks/} -- Reproducible analysis (14 notebooks)
    \item \texttt{configs/} -- YAML configuration files
    \item \texttt{tests/} -- Unit and integration tests (15 test files)
\end{itemize}

\section{Extended Cross-Validation Results}

\subsection{Ten-Fold Time Series Cross-Validation (Germany)}

\input{../reports/tables/draft_table_e1.tex}

\subsection{Multi-Seed Experiment Results}

\input{../reports/tables/draft_table_e2.tex}

\section{Statistical Significance Testing}

\subsection{Diebold-Mariano Test Results}

\input{../reports/tables/draft_table_f1.tex}

\subsection{Paired t-Test for Cost Savings}

\input{../reports/tables/draft_table_f2.tex}

\subsection{Bootstrap Confidence Intervals}

\input{../reports/tables/draft_table_f3.tex}

\section{Hyperparameter Sensitivity Analysis}

\subsection{LightGBM Sensitivity}

\input{../reports/tables/draft_table_g1.tex}

\textbf{Key Finding}: Learning rate is the most sensitive parameter; optimal range is 0.03--0.08.

\subsection{LSTM Sensitivity}

\input{../reports/tables/draft_table_g2.tex}

\subsection{Optimization Parameters}

\input{../reports/tables/draft_table_g3.tex}

\section{Computational Resources}

\subsection{Training Time Breakdown}

\input{../reports/tables/draft_table_h1.tex}

\subsection{Inference Latency}

\input{../reports/tables/draft_table_h2.tex}

\section{Regional Comparison (Germany vs USA)}

\subsection{Forecasting Performance by Region}

\input{../reports/tables/draft_table_i1.tex}

Note: Absolute RMSE differs due to grid scale (DE: about 50 GW peak, US-MISO: about 120 GW peak). Normalized metrics (MAPE, $R^2$) enable fair comparison.

\subsection{Optimization Impact by Region}

\input{../reports/tables/draft_table_i2.tex}

\subsection{Transfer Learning Results}

\input{../reports/tables/draft_table_i3.tex}

Finding: Fine-tuning with 5,000 local samples recovers more than 99\% of native performance.

\section{Seasonal Performance Analysis}

\subsection{Monthly Performance Breakdown (Germany, Load)}

\input{../reports/tables/draft_table_j1.tex}

Seasonal Pattern: Winter months (Dec--Feb) show +15--20\% higher RMSE due to heating demand variability.

\subsection{Day-of-Week Performance (Germany, Load)}

\input{../reports/tables/draft_table_j2.tex}

Finding: Monday transitions from weekend patterns cause +10\% error; weekdays are most predictable.

\subsection{Hour-of-Day Performance (Germany, Load)}

\input{../reports/tables/draft_table_j3.tex}

Key Insight: Morning ramp (06:00--09:00) and evening peak (18:00--21:00) account for 45\% of total forecast error.

\section{Error Distribution Analysis}

\subsection{Error Percentiles (Germany, GBM)}

\input{../reports/tables/draft_table_k1.tex}

\subsection{Error Normality Tests}

\input{../reports/tables/draft_table_k2.tex}

Implication: Non-Gaussian errors justify conformal prediction over parametric intervals.

\subsection{Autocorrelation of Residuals}

\input{../reports/tables/draft_table_k3.tex}

Finding: Significant residual autocorrelation suggests potential for error correction models (future work).

\section{SHAP Feature Importance Analysis}

\subsection{Top 20 Features by SHAP Value (Load Forecasting, Germany)}

\input{../reports/tables/draft_table_l1.tex}

\subsection{Feature Importance Comparison Across Targets}

\input{../reports/tables/draft_table_l2.tex}

\textbf{Key Insights}:
\begin{itemize}
    \item Load: dominated by lagged values and time encoding (daily/weekly cycles).
    \item Wind: weather features (wind speed, pressure) account for 35.8\% importance.
    \item Solar: time encoding captures solar angle; weather (cloud cover) remains influential.
\end{itemize}

\section{Conformal Prediction Detailed Results}

\subsection{Coverage by Nominal Level}

\input{../reports/tables/draft_table_m1.tex}

\subsection{Coverage by Forecast Horizon}

\input{../reports/tables/draft_table_m2.tex}

\subsection{Adaptive Conformal Performance}

\input{../reports/tables/draft_table_m3.tex}

\section{Supplementary Figures}

\subsection{Figure Inventory}

\input{../reports/tables/draft_table_n1.tex}

\subsection{Figure Descriptions}

\textbf{Figure N1--N3}: Forecast scatter plots showing predicted vs actual values with 1:1 reference lines, regression lines with 95\% CI bands, and $R^2$/RMSE annotations. Points are color-encoded by hour-of-day.

\textbf{Figure N4}: Error histograms with kernel density overlays and normal reference curves, including mean, standard deviation, skewness, and kurtosis annotations.

\textbf{Figure N5}: Residual autocorrelation function showing lags 0--48 hours with 95\% confidence bands and significant lags highlighted.

\textbf{Figure N6--N8}: SHAP beeswarm plots with features ranked by mean $|\mathrm{SHAP}|$; color indicates feature value and horizontal position indicates impact on prediction.

\textbf{Figure N9}: Feature interaction heatmap for SHAP interaction values; color intensity indicates interaction strength.

\textbf{Figure N10}: Conformal calibration curves plotting empirical coverage vs nominal level, with separate curves for each target and a diagonal reference for perfect calibration.

\textbf{Figure N15}: Cost-carbon Pareto frontier with dominated strategies marked, GridPulse operating point highlighted, and marginal rate of substitution annotations.

\section{Detailed Model Architectures}

\subsection{LightGBM Final Configuration}

\begin{verbatim}
model_type: lightgbm
objective: regression
metric: rmse
boosting_type: gbdt
n_estimators: 1000
learning_rate: 0.05
max_depth: 8
num_leaves: 64
min_data_in_leaf: 20
subsample: 0.8
subsample_freq: 1
colsample_bytree: 0.8
reg_alpha: 0.1
reg_lambda: 0.1
random_state: 42
early_stopping_rounds: 50
verbose: -1
\end{verbatim}

\textbf{Table O1}: LightGBM Configuration.

\subsection{LSTM Architecture}

\begin{verbatim}
LSTMForecaster(
  (lstm): LSTM(
    input_size=93,
    hidden_size=256,
    num_layers=3,
    batch_first=True,
    dropout=0.3,
    bidirectional=False
  )
  (fc): Sequential(
    (0): Linear(in_features=256, out_features=128)
    (1): ReLU()
    (2): Dropout(p=0.3)
    (3): Linear(in_features=128, out_features=24)
  )
)

Total Parameters: 1,423,896
Trainable Parameters: 1,423,896
\end{verbatim}

\textbf{Table O2}: LSTM Architecture Summary.

\subsection{TCN Architecture}

\begin{verbatim}
TCNForecaster(
  (tcn): TemporalConvNet(
    (network): Sequential(
      (0): TemporalBlock(in=93, out=128, k=5, d=1)
      (1): TemporalBlock(in=128, out=128, k=5, d=2)
      (2): TemporalBlock(in=128, out=128, k=5, d=4)
      (3): TemporalBlock(in=128, out=64, k=5, d=8)
    )
  )
  (fc): Linear(in_features=64, out_features=24)
)

Total Parameters: 558,232
Trainable Parameters: 558,232
Receptive Field: 145 timesteps
\end{verbatim}

\textbf{Table O3}: TCN Architecture Summary.

\section{Reproducibility Checklist}

\subsection{ML Reproducibility Checklist}

\input{../reports/tables/draft_table_p1.tex}

\subsection{Data Sheet}

\input{../reports/tables/draft_table_p2.tex}

\subsection{Model Card}

\input{../reports/tables/draft_table_p3.tex}

\end{document}
